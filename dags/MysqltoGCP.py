"""
MysqltoGCP
DAG auto-generated by Astro Cloud IDE.
"""

from airflow.decorators import dag
from astro import sql as aql
import pandas as pd
import pendulum




@aql.dataframe(task_id="python_1")
def python_1_func():
    import airflow
    import pendulum
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.providers.google.cloud.operators.gcs import GCSCreateBlobOperator
    
    # Database connection parameters
    host_name = '51.15.49.30'
    user_name = 'ihe_vish_readonly'
    password = 'aGT_BJHPW8-K2!Be3i3-eXYd'
    database_name = 'itsmybike-production'
    
    # Google Cloud Storage configuration
    gcs_bucket_name = 'test_data_vish19feb2024'
    gcs_file_name = f'data_{datetime.now().strftime("%Y%m%d%H%M%S")}.csv'
    
    def query_mysql_and_upload_to_gcs():
        # Create a connection to the database
        connection = pymysql.connect(host=host_name,
                                      user=user_name,
                                      password=password,
                                      database=database_name)
    
        # SQL query to execute
        query = "SELECT * FROM `itsmybike-production`.bikestores Limit 100;"
    
        # Load data into a pandas DataFrame
        df = pd.read_sql(query, connection)
    
        # Close the connection
        connection.close()
    
        # Convert the DataFrame to a CSV string
        csv_string = df.to_csv(index=False)
    
        # Upload the CSV string to GCS
        upload_to_gcs(csv_string, gcs_bucket_name, gcs_file_name)
    
    def upload_to_gcs(csv_string, bucket_name, file_name):
        # Create a GCS client
        client = storage.Client()
    
        # Create a bucket if it doesn't exist
        bucket = client.get_bucket(bucket_name)
        if not bucket:
            bucket = client.create_bucket(bucket_name)
    
        # Create a blob in the bucket
        blob = bucket.blob(file_name)
    
        # Upload the CSV string to the blob
        blob.upload_from_string(csv_string)
    
    # Define the DAG
    dag = DAG(
        'query_mysql_and_upload_to_gcs',
        start_date=datetime(2023, 2, 19),
        schedule_interval=None,
    )
    
    
    # Define the tasks
    query_mysql_and_upload_to_gcs_task = PythonOperator(
        task_id='query_mysql_and_upload_to_gcs',
        python_callable=query_mysql_and_upload_to_gcs,
        dag=dag,
    )
    
    gcs_create_blob_task = GCSCreateBlobOperator(
        task_id='gcs_create_blob',
        bucket_name=gcs_bucket_name,
        blob_name=gcs_file_name,
        source_data=csv_string,
        dag=dag,
    )
    
    # Set the task dependencies
    query_mysql_and_upload_to_gcs_task >> gcs_create_blob_task

@aql.dataframe(task_id="python_2")
def python_2_func():
    pip install --upgrade pendulum

default_args={
    "owner": "Vishwanath k,Open in Cloud IDE",
}

@dag(
    default_args=default_args,
    schedule="0 0 * * *",
    start_date=pendulum.from_format("2024-02-22", "YYYY-MM-DD").in_tz("UTC"),
    catchup=False,
    owner_links={
        "Vishwanath k": "mailto:khvishwanathan@gmail.com",
        "Open in Cloud IDE": "https://cloud.astronomer.io/clssrtaa207rf01mj3xddzybg/cloud-ide/clswsbl5a005u01m1bvutbfzg/clswsc7nw00dt01n7ri2jswrc",
    },
)
def MysqltoGCP():
    python_1 = python_1_func()

    python_2 = python_2_func()

dag_obj = MysqltoGCP()
